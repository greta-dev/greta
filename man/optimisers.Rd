% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optimisers.R
\name{optimisers}
\alias{optimisers}
\alias{nelder_mead}
\alias{bfgs}
\alias{powell}
\alias{momentum}
\alias{cg}
\alias{newton_cg}
\alias{l_bfgs_b}
\alias{tnc}
\alias{cobyla}
\alias{slsqp}
\alias{gradient_descent}
\alias{adadelta}
\alias{adagrad}
\alias{adagrad_da}
\alias{adam}
\alias{adamax}
\alias{ftrl}
\alias{proximal_gradient_descent}
\alias{proximal_adagrad}
\alias{nadam}
\alias{rms_prop}
\title{optimisation methods}
\usage{
nelder_mead(
  objective_function = NULL,
  initial_simplex = NULL,
  initial_vertex = NULL,
  step_sizes = NULL,
  objective_at_initial_simplex = NULL,
  objective_at_initial_vertex = NULL,
  func_tolerance = 1e-08,
  position_tolerance = 1e-08,
  parallel_iterations = 1L,
  reflection = NULL,
  expansion = NULL,
  contraction = NULL,
  shrinkage = NULL,
  name = NULL
)

bfgs(
  value_and_gradients_function = NULL,
  initial_position = NULL,
  tolerance = 1e-08,
  x_tolerance = 0L,
  f_relative_tolerance = 0L,
  initial_inverse_hessian_estimate = NULL,
  parallel_iterations = 1L,
  stopping_condition = NULL,
  validate_args = TRUE,
  max_line_search_iterations = 50L,
  f_absolute_tolerance = 0L,
  name = NULL
)

powell()

momentum()

cg()

newton_cg()

l_bfgs_b()

tnc()

cobyla()

slsqp()

gradient_descent(learning_rate = 0.01, momentum = 0, nesterov = FALSE)

adadelta(learning_rate = 0.001, rho = 1, epsilon = 1e-08)

adagrad(learning_rate = 0.8, initial_accumulator_value = 0.1, epsilon = 1e-08)

adagrad_da(
  learning_rate = 0.8,
  global_step = 1L,
  initial_gradient_squared_accumulator_value = 0.1,
  l1_regularization_strength = 0,
  l2_regularization_strength = 0
)

adam(
  learning_rate = 0.1,
  beta_1 = 0.9,
  beta_2 = 0.999,
  amsgrad = FALSE,
  epsilon = 1e-08
)

adamax(learning_rate = 0.001, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-07)

ftrl(
  learning_rate = 1,
  learning_rate_power = -0.5,
  initial_accumulator_value = 0.1,
  l1_regularization_strength = 0,
  l2_regularization_strength = 0,
  l2_shrinkage_regularization_strength = 0,
  beta = 0
)

proximal_gradient_descent(
  learning_rate = 0.01,
  l1_regularization_strength = 0,
  l2_regularization_strength = 0
)

proximal_adagrad(
  learning_rate = 1,
  initial_accumulator_value = 0.1,
  l1_regularization_strength = 0,
  l2_regularization_strength = 0
)

nadam(learning_rate = 0.001, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-07)

rms_prop(
  learning_rate = 0.1,
  rho = 0.9,
  momentum = 0,
  epsilon = 1e-10,
  centered = FALSE
)
}
\arguments{
\item{learning_rate}{the size of steps (in parameter space) towards the
optimal value. Default value 0.01}

\item{momentum}{hyperparameter that accelerates gradient descent in the
relevant direction and dampens oscillations. Defaults to 0, which is
vanilla gradient descent.}

\item{nesterov}{Whether to apply Nesterov momentum. Defaults to FALSE.}

\item{rho}{the decay rate}

\item{epsilon}{a small constant used to condition gradient updates}

\item{initial_accumulator_value}{initial value of the 'accumulator' used to
tune the algorithm}

\item{global_step}{the current training step number}

\item{initial_gradient_squared_accumulator_value}{initial value of the
accumulators used to tune the algorithm}

\item{l1_regularization_strength}{L1 regularisation coefficient (must be 0 or
greater)}

\item{l2_regularization_strength}{L2 regularisation coefficient (must be 0 or
greater)}

\item{beta_1}{exponential decay rate for the 1st moment estimates}

\item{beta_2}{exponential decay rate for the 2nd moment estimates}

\item{amsgrad}{Boolean. Whether to apply AMSGrad variant of this algorithm
from the paper "On the Convergence of Adam and beyond". Defaults to FALSE.}

\item{learning_rate_power}{power on the learning rate, must be 0 or less}

\item{l2_shrinkage_regularization_strength}{A float value, must be greater
than or equal to zero. This differs from L2 above in that the L2 above is
a stabilization penalty, whereas this L2 shrinkage is a magnitude penalty.
When input is sparse shrinkage will only happen on the active weights.}

\item{beta}{A float value, representing the beta value from the paper by
\href{https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41159.pdf}{McMahan et al 2013}. Defaults to 0}

\item{centered}{Boolean. If TRUE, gradients are normalized by the estimated
variance of the gradient; if FALSE, by the uncentered second moment.
Setting this to TRUE may help with training, but is slightly more
expensive in terms of computation and memory. Defaults to FALSE.}
}
\value{
an \code{optimiser} object that can be passed to \code{\link[=opt]{opt()}}.
}
\description{
Functions to set up optimisers (which find parameters that
maximise the joint density of a model) and change their tuning parameters,
for use in \code{\link[=opt]{opt()}}. For details of the algorithms and how to
tune them, see the \href{https://www.tensorflow.org/api_docs/python/tf/keras/optimizers}{TensorFlow optimiser docs}, or the \href{https://www.tensorflow.org/probability/api_docs/python/tfp/optimizer}{Tensorflow Probability optimiser docs}.
}
\details{
The optimisers \code{powell()}, \code{cg()}, \code{newton_cg()},
\code{l_bfgs_b()}, \code{tnc()}, \code{cobyla()}, and \code{slsqp()} are
now defunct. They will error when called in greta 0.5.0. This are removed
because they are no longer available in TensorFlow 2.0. Note that
optimiser \code{momentum()} has been replaced with \code{gradient_descent()}
}
\note{
This optimizer isn't supported in TF2, so proceed with caution. See
the \href{https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/AdagradDAOptimizer}{TF docs on AdagradDAOptimiser} for more detail.

This optimizer isn't supported in TF2, so proceed with caution. See
the \href{https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/ProximalGradientDescentOptimizer}{TF docs on AdagradDAOptimiser} for more detail.

This optimizer isn't supported in TF2, so proceed with caution. See
the \href{https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/ProximalAdagradOptimizer}{TF docs on AdagradDAOptimiser} for more detail.
}
\examples{
\dontrun{
# use optimisation to find the mean and sd of some data
x <- rnorm(100, -2, 1.2)
mu <- variable()
sd <- variable(lower = 0)
distribution(x) <- normal(mu, sd)
m <- model(mu, sd)

# configure optimisers & parameters via 'optimiser' argument to opt
opt_res <- opt(m, optimiser = bfgs())

# compare results with the analytic solution
opt_res$par
c(mean(x), sd(x))
}
}
