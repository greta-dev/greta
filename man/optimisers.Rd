% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optimisers.R
\name{optimisers}
\alias{optimisers}
\alias{nelder_mead}
\alias{powell}
\alias{cg}
\alias{bfgs}
\alias{newton_cg}
\alias{l_bfgs_b}
\alias{tnc}
\alias{cobyla}
\alias{slsqp}
\alias{gradient_descent}
\alias{adadelta}
\alias{adagrad}
\alias{adagrad_da}
\alias{momentum}
\alias{adam}
\alias{ftrl}
\alias{proximal_gradient_descent}
\alias{proximal_adagrad}
\alias{rms_prop}
\title{optimisation methods}
\usage{
nelder_mead()

powell()

cg()

bfgs()

newton_cg()

l_bfgs_b(maxcor = 10, maxls = 20)

tnc(max_cg_it = -1, stepmx = 0, rescale = -1)

cobyla(rhobeg = 1)

slsqp()

gradient_descent(learning_rate = 0.01)

adadelta(learning_rate = 0.001, rho = 1, epsilon = 1e-08)

adagrad(learning_rate = 0.8, initial_accumulator_value = 0.1)

adagrad_da(learning_rate = 0.8, global_step = 1L,
  initial_gradient_squared_accumulator_value = 0.1,
  l1_regularization_strength = 0, l2_regularization_strength = 0)

momentum(learning_rate = 0.001, momentum = 0.9, use_nesterov = TRUE)

adam(learning_rate = 0.1, beta1 = 0.9, beta2 = 0.999,
  epsilon = 1e-08)

ftrl(learning_rate = 1, learning_rate_power = -0.5,
  initial_accumulator_value = 0.1, l1_regularization_strength = 0,
  l2_regularization_strength = 0)

proximal_gradient_descent(learning_rate = 0.01,
  l1_regularization_strength = 0, l2_regularization_strength = 0)

proximal_adagrad(learning_rate = 1, initial_accumulator_value = 0.1,
  l1_regularization_strength = 0, l2_regularization_strength = 0)

rms_prop(learning_rate = 0.1, decay = 0.9, momentum = 0,
  epsilon = 1e-10)
}
\arguments{
\item{maxcor}{maximum number of 'variable metric corrections' used to define
the approximation to the hessian matrix}

\item{maxls}{maximum number of line search steps per iteration}

\item{max_cg_it}{maximum number of hessian * vector evaluations per iteration}

\item{stepmx}{maximum step for the line search}

\item{rescale}{log10 scaling factor used to trigger rescaling of objective}

\item{rhobeg}{reasonable initial changes to the variables}

\item{learning_rate}{the size of steps (in parameter space) towards the
optimal value}

\item{rho}{the decay rate}

\item{epsilon}{a small constant used to condition gradient updates}

\item{initial_accumulator_value}{initial value of the 'accumulator' used to
tune the algorithm}

\item{global_step}{the current training step number}

\item{initial_gradient_squared_accumulator_value}{initial value of the accumulators used to
tune the algorithm}

\item{l1_regularization_strength}{L1 regularisation coefficient (must be 0 or greater)}

\item{l2_regularization_strength}{L2 regularisation coefficient (must be 0 or greater)}

\item{momentum}{the momentum of the algorithm}

\item{use_nesterov}{whether to use Nesterov momentum}

\item{beta1}{exponential decay rate for the 1st moment estimates}

\item{beta2}{exponential decay rate for the 2nd moment estimates}

\item{learning_rate_power}{power on the learning rate, must be 0 or less}
}
\value{
an \code{optimiser} object that can be passed to \code{\link{opt}}.
}
\description{
Functions to set up optimisers (which find parameters that
  maximise the joint density of a model) and change their tuning parameters,
  for use in \code{\link{opt}()}. For details of the algorithms and how to
  tune them, see the
  \href{https://www.tensorflow.org/api_guides/python/train#Optimizers}{TensorFlow
   optimiser docs} or the
  \href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html}{SciPy
   optimiser docs}.
}
\details{
The TensorFlow interface to SciPy optimisers issues runtime warnings
  that some of these methods (\code{nelder_mead()}, \code{powell()},
  \code{cobyla()}) do not use gradient information and that \code{cobyla()}
  'does not support callback'. These can be safely ignored.

The \code{cobyla()} does not provide information about the number of
  iterations for convergence, so these values fo the output are set to NA
}
\examples{
\dontrun{
# use optimisation to find the mean and sd of some data
x <- rnorm(100, -2, 1.2)
mu <- variable()
sd <- variable(lower = 0)
distribution(x) <- normal(mu, sd)
m <- model(mu, sd)

# configure optimisers & parameters via 'optimiser' argument to opt
opt_res <- opt(m, optimiser = bfgs())

# compare results with the analytic solution
opt_res$par
c(mean(x), sd(x))
}
}
